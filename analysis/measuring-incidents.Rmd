---
title: "Measuring Incident Duration"
author: "John Benninghoff"
date: '2025-03-19'
date-modified: '2024-03-19'
categories: SRE
order: 107
output:
  html_notebook:
    theme:
      version: 5
      preset: bootstrap
    css: assets/extra.css
    pandoc_args: --shift-heading-level-by=1
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
---

Analysis to determine thresholds of sample size to reliably detect changes in incident impact (duration) using Monte Carlo simulation.

# Questions/TODO

- [ ] Questions/TODO list here

```{r setup, message = FALSE, warning = FALSE}
library(quantrr)
library(purrr)
library(tibble)
library(dplyr)
```

# Background

In 2021, Google published a report exploring the limitations of using Mean Time to Resolve (MTTR), [Incident Metrics in SRE: Critically Evaluating MTTR and Friends](https://sre.google/resources/practices-and-processes/incident-metrics-in-sre/). Since then,
additional articles and reports have been published that are also critical of MTTR and TTR in
general, explicitly advocating that
"[organizations should stop using MTTX, or TTx data in general](https://resilienceinsoftware.org/news/1157532)". From my experience with cybersecurity
incident data, I believe it is a mistake to abandon measurements of incidents entirely, and instead
ask the question, *under what conditions does incident duration indicate improvement?*

Key observations and arguments from the 2021 Google paper include:

- Incident duration appears to be lognormal (the paper did not test if the lognormal distribution
  was the best fit)
- A Monte Carlo simulation comparing two samples of incidents, one with no change in duration, and
  a second with a 10% reduction in duration, MTTR did not reliably indicate the improvement
- Improvements in MTTR can be observed even when there is no change in the underlying distribution
- Larger sample sizes (more incidents) make detecting the change more reliable - in the original
  simulation, the sample sizes ranged from approximately 50-300
- Median and 95th percentile don't work well either
- Geometric mean does just work, but only with a sample size of 500+
- Measuring incident duration doesn't account for changes in incident frequency

The article also calls out some exceptions where measuring duration is effective: first, when your
data is large enough, as with Backblaze statistics on hard disk reliability, and second, when the
change in duration is large enough, such as an 80% reduction in duration.

Subsequent reports from [VOID](https://www.thevoid.community) referenced the Google paper and argued:

- Incident duration is long-tailed and appear lognormal (2021, 2022)
- "MTTx are Shallow Data", an oversimplification of a complex, messy event (2021)
- Organizations should stop using MTTx and TTx data (2021, 2022, 2024)
- Incident duration appears lognormal for some organizations, but not for others (2022)
- Replicated the Monte Carlo simulation from the Google paper (2022)

In the cybersecurity domain, the [Cyentia Institute](https://www.cyentia.com) has published reports
analyzing breach data, looking at both frequency and impact (cost). Their
[IRIS 2022](https://www.cyentia.com/wp-content/uploads/IRIS-2022_Cyentia.pdf) report found that,
like outage incident duration, security breach losses closely follow a lognormal distribution. Like
the Google and VOID reports, the IRIS also points out that the mean is a poor indicator of a
'typical' loss, the median or geometric mean are better summaries, and organizations should also be
concerned with tail risk - extreme events at the 90, 95, and 99th percentile.

Additionally, despite the challenges with using lognormal data, the
[2024 Ransomware Report](https://www.cyentia.com/iris-ransomware/) showed that losses from
ransomware have significantly increased between 2019 and 2023.

# Analysis

While I agree that the mean (average) is misleading and should not be used with incident impact data
(duration or costs), I believe it is a mistake to stop using impact data entirely. A Monte Carlo
simulation can help answer the underlying question raised by the Google research, "under what
conditions does incident duration indicate improvement?" Put differently, how large of a sample size
is needed to detect a given level of improvement (reduction) in incident duration?

## Assumptions

While the 2022 VOID Report challenged the notion that incident duration is lognormal, the Google
report and my own experience suggest otherwise: similar to cybersecurity impact, incident duration
closely follows a lognormal distribution. In my own work, I have found that incidents that are
qualitatively different may not be lognormal, and that could well be what was happening in the VOID
report, but we lack the context to know for sure. To simplify the analysis, the incident frequency
will be held constant. (Side note: Cyentia found that cybersecurity breach frequency followed a
Poisson lognormal distribution)

## Approach

To determine the threshold of incidents needed to detect changes in duration, we follow a similar
methodology as the Google paper:

1. Randomly sample from an artificial but representative lognormal distribution of incident duration
2. Adjust the duration of one of the populations by a set percentage to simulate improvement
3. Determine if there is a statistically significant improvement in incident duration

For step 3, we use the geometric mean and
[Student's t-test](https://en.wikipedia.org/wiki/Student's_t-test) on the samples, after
log-transforming the data. From my research and understanding, this is a reasonable approach if the
underlying distributions are truly lognormal, which is true by design.

To determine a representative distribution, we can borrow data from the Google report, which lists
the p95 and p50 for three companies - we'll infer p05 and the lognormal parameters using data from
Company B:

```{r company_b}
lnorm_param(9, 528, 67)
```

Using trial and error, we find that 9 minutes is the closest match to p05 when the median is 67
minutes. From this, we can create a representative distribution:

```{r meanlog}
meanlog <- 4.25
sdlog <- 1.25
qlnorm(c(0.05, 0.5, 0.95), meanlog = meanlog, sdlog = sdlog)
```

This is reasonably close to the Company B data, and passes the sniff test: 5% of incidents resolved
in less than 9 minutes, 50% resolved in 70 minutes or less, and 95% resolved under 548 minutes (9
hours 8 minutes).

## Simulations

First, test at the 10% improvement level, with a sample size ($N$) of 100, which is about 1 year's
worth of incidents. A single test is successful if the experimental group has a lower geometric mean
and a positive t-test (p < 0.05) on the log-transformed data.

```{r single_test}
single_test <- function(n, improvement) {
  control <- rlnorm(n, meanlog = meanlog, sdlog = sdlog)
  experiment <- rlnorm(n, meanlog = meanlog, sdlog = sdlog) * (1 - improvement)

  list(gmean(control), gmean(experiment), t.test(log(control), log(experiment)))
}

single_test(100, 0.1)
```

For most tests, the p-value is well above 0.05. How often does the test succeed? We'll set the bar
slightly higher than the Google paper and target a 95% success rate.

We'll use a slightly different function that is more easily converted into a data frame, and run
only 10,000 trials to speed things up:

```{r simulate_test}
test_change <- function(n, improvement) {
  control <- rlnorm(n, meanlog = meanlog, sdlog = sdlog)
  experiment <- rlnorm(n, meanlog = meanlog, sdlog = sdlog) * (1 - improvement)

  c(
    control_gmean = gmean(control), experiment_gmean = gmean(experiment),
    p_value = t.test(log(control), log(experiment))$p.value
  )
}

simulate_test <- function(n, improvement, trials = 10000, p_value_target = 0.05) {
  map(1:trials, \(i) as_tibble_row(test_change(n, improvement))) |>
    list_rbind() |>
    mutate(
      success = (.data$control_gmean > .data$experiment_gmean) & (.data$p_value < p_value_target)
    )
}

results <- simulate_test(100, 0.1)
head(results, 10)
mean(results$success)
```

For a sample size of 100, we successfully detect a statistically significant change about 8% of the
time. We can try larger sample sizes:

```{r larger_samples}
simulate_test(1000, 0.1) |>
  pull(success) |>
  mean()

simulate_test(2000, 0.1) |>
  pull(success) |>
  mean()

simulate_test(4000, 0.1) |>
  pull(success) |>
  mean()
```

A sample size of 4000 is needed to detect a significant change. For a single organization, it would
take about 80 years (40 before, 40 after) to gather the data. However, the same volume of data could
be gathered from 80 different organizations (40 in each group) in a year.

We can also try larger changes:

```{r larger_change}
simulate_test(100, 0.25) |>
  pull(success) |>
  mean()

simulate_test(100, 0.50) |>
  pull(success) |>
  mean()
```

Interestingly, while the Google paper notes that it would take a very large change (80% reduction)
to be detected through measuring incident duration, we find that a 50% reduction is clearly
detectable over two years of data.

# Discussion

Discuss results here.
