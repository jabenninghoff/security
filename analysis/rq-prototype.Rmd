---
title: "Risk Quantification Prototype"
author: "John Benninghoff"
date: '2024-07-23'
date-modified: '2024-11-01'
categories: risk quantification
order: 103
output:
  html_notebook:
    theme:
      version: 5
      preset: bootstrap
    css: assets/extra.css
    pandoc_args: --shift-heading-level-by=1
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
---

Prototype Risk Quantification tool for my SIRAcon 2024 talk, "[UnFAIR: Simplifying and Expanding Technology Risk Quantification](https://www.information-safety.org/2024/08/29/siracon-2024/)."

# Questions/TODO

- [x] Figure out how to calculate log normal `meanlog` and `sdlog` from expert estimation (see [Calculating SD for normal distribution with only mean and 5% and 95% quantile values](https://stats.stackexchange.com/questions/158110/calculating-sd-for-normal-distribution-with-only-mean-and-5-and-95-quantile-va))
- [x] Log Scale Loss Exceedance Curves: add a log10 scale LEC plot using `dplyr::percent_rank()` instead of `ggplot2::stat_ecdf()`, see examples from [Elliot Murphy](https://rpubs.com/emurphy77/fair-example) and [David Severski](https://github.com/davidski/evaluator/blob/main/R/common_graphs.R#L286)

```{r setup, message = FALSE, warning = FALSE}
library(quantrr)
library(ggplot2)
library(jbplot)
library(readxl)
library(janitor)
library(dplyr)
library(formattable)
library(purrr)
library(tibble)
library(plotly)
```

# Example Data

Generate realistic example data to populate the Excel spreadsheet. As discovered in practice, while
the initial risks ("cybersecurity breach" and "technology outage") are raised by the technology
team, we find that the business risks (here represented by "loss of customer") are significantly
larger, caused by the functional obsolescence of the inventory system.

## Frequency

Experts are asked to estimate the frequency of an adverse event, which is used as the average
arrival rate for a Poisson distribution ($\lambda$).

From my [breach rate analysis](breach-rates.Rmd), the largest firms experience a cybersecurity
breach about once every three years ($\lambda$ = 0.347). A reasonably high breach rate for a system
is between 0.2 and 0.33.

From my own experience, an unstable system might experience between 1-4 technology outages per year;
a $\lambda$ of 2 fits this nicely:

```{r outage_freq}
ggplot(data.frame(x = rpois(1e5, 2)), aes(x)) +
  geom_hist_bw(binwidth = 1) +
  scale_x_continuous(breaks = scales::breaks_width(1, offset = 1)) +
  coord_cartesian(xlim = c(0, 10)) +
  labs(x = NULL, y = NULL) +
  theme_quo(minor = FALSE)
```

For the third risk, loss of customer, the firm has already experienced a loss, and expects this to
increase. For loss of customer we also choose a $\lambda$ of 2.

## Impact

Drawing from the [IRIS 2022](https://www.cyentia.com/wp-content/uploads/IRIS-2022_Cyentia.pdf)
report, the loss magnitude of a cybersecurity breach follows a log-normal distribution with a mean
($\mu$) of 12.56 and a standard deviation ($\sigma$) of 3.07, which gives a log-transformed
lognormal curve of:

```{r breach_impact_dist}
ggplot(data.frame(x = c(0, 25)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 12.56, sd = 3.07)) +
  labs(x = NULL, y = NULL) +
  theme_quo()
```

Note: from
[Making sense of the rlnorm() function in R](https://msalganik.wordpress.com/2017/01/21/making-sense-of-the-rlnorm-function-in-r/):

> `rlnorm(n = 1000000, meanlog = 7, sdlog = 75)` and `exp(rnorm(n = 1000000, mean = 7, sd = 75))`
> produce the same result

An expert estimate matching this outcome for a breach would have low (5%), high (95%) and most
likely values of:

```{r breach_impact}
qlnorm(c(0.05, 0.95, 0.5), meanlog = 12.56, sdlog = 3.07)
```

For a technology outage, a mean of 0.1 and a standard deviation of 1.7 are reasonable choices for
parameters that fit the data for outage duration from the
[2022 VOID Report](https://mcusercontent.com/ad9b3a31c069c03a14227a79c/files/29e138f4-05b7-f560-a7d5-c337538a93eb/2022_Void_Report.pdf):

```{r outage_duration_dist}
ggplot(data.frame(x = rlnorm(1e3, meanlog = 0.1, sdlog = 1.7)), aes(x)) +
  geom_hist_bw(binwidth = 1) +
  coord_cartesian(xlim = c(0, 80)) +
  labs(x = NULL, y = NULL) +
  theme_quo(minor = FALSE)
```

This gives low (5%), high (95%) and most likely values of:

```{r outage_duration}
qlnorm(c(0.05, 0.95, 0.5), meanlog = 0.1, sdlog = 1.7)
```

This fits with my own experience; most outages last around an hour, and 95% of outages are resolved
in 18 hours or less. From this, we can calculate a reasonable outage cost by multiplying by a
per-hour rate. We assume that the main cost is caused by response and overtime paid out to catch up
after the system is restored, $10K/hour.

```{r outage_impact}
qlnorm(c(0.05, 0.95, 0.5), meanlog = 0.1, sdlog = 1.7) * 10000
```

For loss of customer, we estimate the low (5%) and high (95%) values at \$100K and \$20M
respectively; these are reasonable choices given the size of the firm. We can test this by
calculating the most likely value, after determining the mean and standard deviation using the
approach from this
[StackExchange Post](https://stats.stackexchange.com/questions/158110/calculating-sd-for-normal-distribution-with-only-mean-and-5-and-95-quantile-va):

The mean is calculated using: $\mu = \large \frac{\log(high) - \log(low)}{2}$

```{r meanlog_customer}
meanlog_customer <- ((log(20000000) - log(100000)) / 2) + log(100000)
meanlog_customer
```

The standard deviation is calculated using:
$\DeclareMathOperator{\qnorm}{qnorm} \large \sigma = \frac{\log(high) - \log(low)}{2 \times \qnorm(0.95)}$

```{r sdlog_customer}
sdlog_customer <- (log(20000000) - log(100000)) / (2 * qnorm(0.95))
sdlog_customer
```

From this we get a customer impact range of:

```{r customer_impact}
qlnorm(c(0.05, 0.95, 0.5), meanlog = meanlog_customer, sdlog = sdlog_customer)
```

# Import

Import data from Excel template, loading each tab into its own data frame. The data in
`prototype.xlsx` is based on the examples provided above.

Risk descriptions:

```{r risks}
risks <- read_xlsx("data/prototype.xlsx", sheet = "Risks") |>
  clean_names()

risks |>
  formattable(align = "l")
```

Risk estimates:

```{r estimates}
estimates <- read_xlsx("data/prototype.xlsx", sheet = "Estimates") |>
  clean_names() |>
  rename(
    lambda = frequency_per_yer, p05 = low_5_percent, p95 = high_95_percent, p50 = most_likely
  )

estimates |>
  mutate(across(p05:p50, ~ currency(.x, digits = 0L))) |>
  formattable(align = "l")
```

# Consensus Estimate

Using a simple average of all experts that provided an estimate (not blank/NA), this gives us a
consensus estimate for the three risks of:

```{r consensus}
consensus <- estimates |>
  group_by(risk) |>
  summarize(across(lambda:p50, ~ mean(.x, na.rm = TRUE)))

consensus |>
  mutate(across(p05:p50, ~ currency(.x, digits = 0L))) |>
  formattable(align = "l")
```

# Forecast

Forecast risk using Monte Carlo simulation.

First calculate the parameters of the log normal distributions for each risk. Also calculate the
differential of the most likely outcome (50th percentile) to provide insight into estimate accuracy:

```{r consensus_params}
lnorm_param

consensus_params <- consensus |>
  mutate(as_tibble(lnorm_param(.data$p05, .data$p95, .data$p50)))

consensus_params |>
  mutate(across(p05:p50, ~ currency(.x, digits = 0L))) |>
  formattable(align = "l")
```

Run the simulation:

```{r forecast}
calc_risk

forecast <- consensus_params |>
  select(c("risk", "lambda", "meanlog", "sdlog")) |>
  pmap(calc_risk) |>
  list_rbind()

forecast |>
  group_by(risk) |>
  summarize(avg_events = mean(events), avg_losses = mean(losses)) |>
  mutate(avg_losses = currency(avg_losses, digits = 0L)) |>
  formattable(align = "l")
```

# Histograms

Use histograms to compare risks and visualize overall risk.

```{r risk_hist}
forecast |>
  ggplot(aes(losses)) +
  facet_grid(vars(risk)) +
  geom_hist_bw(bins = 100) +
  scale_x_log10(labels = scales::label_currency(scale_cut = scales::cut_short_scale())) +
  labs(x = NULL, y = NULL) +
  theme_quo()
```

Combined risk:

```{r combined_hist}
forecast |>
  ggplot(aes(losses, fill = risk)) +
  geom_histogram(bins = 100) +
  scale_x_log10(labels = scales::label_currency(scale_cut = scales::cut_short_scale())) +
  labs(x = NULL, y = NULL, fill = "Risk") +
  theme_quo()
```

# Loss Exceedance Curves

Plot loss exceedance curves for all risks and combined risk.

First calculate the largest p95 (95% quantile), to limit the x-axis for static plots:

```{r p95_max}
p95_max <- forecast |>
  group_by(risk) |>
  summarize(p95 = quantile(losses, 0.95)) |>
  pull(p95) |>
  max()
```

## By Risk

Plot loss exceedance curves for each risk:

```{r risk_le}
risk_le <- forecast |>
  ggplot(aes(losses)) +
  facet_grid(vars(risk)) +
  stat_ecdf(aes(y = after_stat(1 - y))) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_continuous(labels = scales::label_currency(scale_cut = scales::cut_short_scale())) +
  labs(x = NULL, y = NULL)

risk_le +
  coord_cartesian(xlim = c(0, p95_max)) +
  theme_quo()
```

Interactive plot:

```{r risk_le_plotly}
ggplotly(risk_le + theme_minimal())
```

Plot using percent rank and a log10 scale:

```{r risk_le_log}
risk_le_log <- forecast |>
  group_by(risk) |>
  mutate(probability = 1 - percent_rank(losses)) |>
  filter(losses > 0) |>
  ggplot(aes(losses, probability)) +
  facet_grid(vars(risk)) +
  geom_line() +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_log10(labels = scales::label_currency(scale_cut = scales::cut_short_scale())) +
  labs(x = NULL, y = NULL)

risk_le_log +
  theme_quo()

ggplotly(risk_le_log + theme_minimal())
```

Both methods calculate the same probability of a loss exceeding a given value, but the log10 method is easier to read.

## Combined Risk

Plot loss exceedance curves for combined risk:

```{r combined_le}
combined_le <- forecast |>
  group_by(year) |>
  summarize(total_losses = sum(losses)) |>
  ggplot(aes(total_losses)) +
  stat_ecdf(aes(y = after_stat(1 - y))) +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_continuous(labels = scales::label_currency(scale_cut = scales::cut_short_scale())) +
  labs(x = NULL, y = NULL)

combined_le +
  coord_cartesian(xlim = c(0, p95_max)) +
  theme_quo()
```

Interactive plot:

```{r combined_le_plotly}
ggplotly(combined_le + theme_minimal())
```

Plot using percent rank and a log10 scale:

```{r combined_le_log}
combined_le_log <- forecast |>
  group_by(year) |>
  summarize(total_losses = sum(losses)) |>
  mutate(probability = 1 - percent_rank(total_losses)) |>
  filter(total_losses > 0) |>
  ggplot(aes(total_losses, probability)) +
  geom_line() +
  scale_y_continuous(labels = scales::label_percent()) +
  scale_x_log10(labels = scales::label_currency(scale_cut = scales::cut_short_scale())) +
  labs(x = NULL, y = NULL)

combined_le_log +
  theme_quo()

ggplotly(combined_le_log + theme_minimal())
```
