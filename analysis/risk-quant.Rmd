---
title: "Technology Risk Quantification"
author: "John Benninghoff"
date: '2024-07-23'
date-modified: '2024-07-23'
categories: risk quantification
order: 103
output:
  html_notebook:
    theme:
      version: 5
      preset: bootstrap
    css: assets/extra.css
    pandoc_args: --shift-heading-level-by=1
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
---

Prototype Risk Quantification tool for my SIRAcon 2024 talk, "[UnFAIR: Simplifying and Expanding Technology Risk Quantification](https://web.cvent.com/event/7f49b0a6-bca9-46fd-8245-a2deb671efee/websitePage:23d1376e-2723-411a-910a-0edf87b03015?session=956b9176-13ad-4dac-af3c-e8ccb30bae8a&shareLink=true)."

# Questions/TODO

- [x] Figure out how to calculate log normal `meanlog` and `sdlog` from expert estimation (see [Calculating SD for normal distribution with only mean and 5% and 95% quantile values](https://stats.stackexchange.com/questions/158110/calculating-sd-for-normal-distribution-with-only-mean-and-5-and-95-quantile-va))

```{r setup, message = FALSE, warning = FALSE}
library(ggplot2)
library(jbplot)
```

# Example Data

Generate realistic example data to populate the Excel spreadsheet. As discovered in practice, while
the initial risks ("cybersecurity breach" and "technology outage") are raised by the technology
team, we find that the business risks (here represented by "loss of customer") are significantly
larger, caused by the functional obsolescence of the inventory system.

## Frequency

Experts are asked to estimate the frequency of an adverse event, which is used as the average
arrival rate for a Poisson distribution ($\lambda$).

From my [breach rate analysis](breach-rates.Rmd), the largest firms experience a cybersecurity
breach about once every three years ($\lambda$ = 0.347). A reasonably high breach rate for a system
is between 0.2 and 0.33.

From my own experience, an unstable system might experience between 1-4 technology outages per year;
a $\lambda$ of 2 fits this nicely:

```{r outage_freq}
ggplot(data.frame(x = rpois(1e5, 2)), aes(x)) +
  geom_hist_bw(binwidth = 1) +
  scale_x_continuous(breaks = scales::breaks_width(1, offset = 1)) +
  coord_cartesian(xlim = c(0, 10)) +
  labs(x = NULL, y = NULL) +
  theme_quo(minor = FALSE)
```

For the third risk, loss of customer, the firm has already experienced a loss, and expects this to
increase. For loss of customer we also choose a $\lambda$ of 2.

## Impact

Drawing from the [IRIS 2022](https://www.cyentia.com/wp-content/uploads/IRIS-2022_Cyentia.pdf)
report, the loss magnitude of a cybersecurity breach follows a log-normal distribution with a mean
($\mu$) of 12.56 and a standard deviation ($\sigma$) of 3.07, which gives a log-transformed
lognormal curve of:

```{r breach_impact_dist}
ggplot(data.frame(x = c(0, 25)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 12.56, sd = 3.07)) +
  labs(x = NULL, y = NULL) +
  theme_quo()
```

Note: from
[Making sense of the rlnorm() function in R](https://msalganik.wordpress.com/2017/01/21/making-sense-of-the-rlnorm-function-in-r/):

> `rlnorm(n = 1000000, meanlog = 7, sdlog = 75)` and `exp(rnorm(n = 1000000, mean = 7, sd = 75))`
> produce the same result

An expert estimate matching this outcome for a breach would have low (5%), high (95%) and most
likely values of:

```{r breach_impact}
qlnorm(c(0.05, 0.95, 0.5), meanlog = 12.56, sdlog = 3.07)
```

For a technology outage, a mean of 0.1 and a standard deviation of 1.7 are reasonable choices for
parameters that fit the data for outage duration from the
[2022 VOID Report](https://mcusercontent.com/ad9b3a31c069c03a14227a79c/files/29e138f4-05b7-f560-a7d5-c337538a93eb/2022_Void_Report.pdf):

```{r outage_duration_dist}
ggplot(data.frame(x = rlnorm(1e3, meanlog = 0.1, sdlog = 1.7)), aes(x)) +
  geom_hist_bw(binwidth = 1) +
  coord_cartesian(xlim = c(0, 80)) +
  labs(x = NULL, y = NULL) +
  theme_quo(minor = FALSE)
```

This gives low (5%), high (95%) and most likely values of:

```{r outage_duration}
qlnorm(c(0.05, 0.95, 0.5), meanlog = 0.1, sdlog = 1.7)
```

This fits with my own experience; most outages last around an hour, and 95% of outages are resolved
in 18 hours or less. From this, we can calculate a reasonable outage cost by multiplying by a
per-hour rate. We assume that the main cost is caused by response and overtime paid out to catch up
after the system is restored, $10K/hour.

```{r outage_impact}
qlnorm(c(0.05, 0.95, 0.5), meanlog = 0.1, sdlog = 1.7) * 10000
```

For loss of customer, we estimate the low (5%) and high (95%) values at \$100K and \$20M
respectively; these are reasonable choices given the size of the firm. We can test this by
calculating the most likely value, after determining the mean and standard deviation using the
approach from this
[StackExchange Post](https://stats.stackexchange.com/questions/158110/calculating-sd-for-normal-distribution-with-only-mean-and-5-and-95-quantile-va):

The mean is calculated using: $\mu = \large \frac{\log(high) - \log(low)}{2}$

```{r meanlog_customer}
meanlog_customer <- ((log(20000000) - log(100000)) / 2) + log(100000)
meanlog_customer
```

The standard deviation is calculated using:
$\DeclareMathOperator{\qnorm}{qnorm} \large \sigma = \frac{\log(high) - \log(low)}{2 \times \qnorm(0.95)}$

```{r sdlog_customer}
sdlog_customer <- (log(20000000) - log(100000)) / (2 * qnorm(0.95))
sdlog_customer
```

From this we get a customer impact range of:

```{r customer_impact}
qlnorm(c(0.05, 0.95, 0.5), meanlog = meanlog_customer, sdlog = sdlog_customer)
```

# Import

Import data from Excel template.
