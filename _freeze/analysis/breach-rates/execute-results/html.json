{
  "hash": "84d3ca52d1d23689dd3bf8cd489b784d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Measuring Changes in Breach Rates\"\nauthor: \"John Benninghoff\"\ndate: '2024-06-05'\ndate-modified: '2024-06-06'\ncategories: security differently\norder: 102\noutput:\n  html_notebook:\n    theme:\n      version: 5\n      preset: bootstrap\n    css: assets/extra.css\n    pandoc_args: --shift-heading-level-by=1\n    toc: yes\n    toc_float:\n      collapsed: no\n      smooth_scroll: no\n---\n\n\n\n\nA critical review of using breach frequency as a measure of security success, inspired by [Incident Metrics in SRE: Critically Evaluating MTTR and Friends](https://sre.google/resources/practices-and-processes/incident-metrics-in-sre/).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(poilog)\nlibrary(tibble)\nlibrary(dplyr)\n```\n:::\n\n\n\n\n# Background\n\n[Incident Metrics in SRE: Critically Evaluating MTTR and Friends](https://sre.google/resources/practices-and-processes/incident-metrics-in-sre/) demonstrated\nthe difficulty at detecting changes in MTTR over time. The paper tests a 10% reduction in MTTR using\nMonte Carlo simulation, and found:\n\n> \"Yikes! Even though in the simulation the improvement always worked, 38% of the simulations had\n> the MTTR difference fall below zero for Company A, 40% for Company B, and 20% for Company C.\n> Looking at the absolute change in MTTR, the probability of seeing at least a 15-minute improvement\n> is only 49%, 50%, and 64%, respectively. Even though the product in the scenario worked and\n> shortened incidents, the odds of detecting any improvement at all are well outside the tolerance\n> of 10% random flukes.\"\n\n[Safety Differently](https://www.information-safety.org/2023/10/31/security-differently/) argues\nthat we shouldn't measure success based on the absence of negative events. We can use real data on\ndata breaches and Monte Carlo simulation to explore the question, \"Is an organization's breach rate\na useful metric?\"\n\n# Poisson Log-Normal Model\n\nThe 2022\n[Information Risk Insights Study (IRIS)](https://www.cyentia.com/wp-content/uploads/IRIS-2022_Cyentia.pdf)\nfound that a Poisson log-normal distribution fits the data and provides realistic forecasts for\nyears with multiple breaches, and provides suggested values for mean ($\\mu$) and standard deviation\n($\\sigma$) in Table 1, along with the likelihood of a breach with models that are adjusted for firm\nsize in Table 2, since larger firms are more likely to experience a breach.\n\nUsing the provided parameters from Table 1, we get:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbreach_poilog <- function(mu, sig, runs = 1e6) {\n  breaches <- rpoilog(runs, mu, sig)\n  tibble(\n    \"One or more\" = sum(breaches >= 1) / runs,\n    \"Two or more\" = sum(breaches >= 2) / runs,\n    \"Three or more\" = sum(breaches >= 3) / runs\n  )\n}\n\nbreach_poilog(mu = -2.284585, sig = 0.8690759)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  `One or more` `Two or more` `Three or more`\n          <dbl>         <dbl>           <dbl>\n1         0.129        0.0163         0.00250\n```\n\n\n:::\n:::\n\n\n\n\nThese values are close to the upper bound for a firm in the \\$100M to \\$1B range on Table 2.\n\n# Poisson Model\n\nIf we were to use the number of breaches a firm has experienced as a useful metric, in the best-case\nscenario, we would have a high rate of breaches with a significant reduction after implementing a\nnew control. Unfortunately, the IRIS paper doesn't provide the model parameters for the group with\nthe highest likelihood of a breach, firms with more than $100B in revenue. Additionally, because it\nis easier to understand and calculate improvements, we'd like to use a plain Poisson distribution,\nwhich has a single parameter for average arrival rate, $\\lambda$.\n\nWe can find a reasonable value for $\\lambda$ using trial and error:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbreach_pois <- function(lambda, runs = 1e6) {\n  breaches <- rpois(runs, lambda)\n  tibble(\n    \"One or more\" = sum(breaches >= 1) / runs,\n    \"Two or more\" = sum(breaches >= 2) / runs,\n    \"Three or more\" = sum(breaches >= 3) / runs\n  )\n}\n\nbase_lambda <- 0.347\nbreach_pois(base_lambda)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  `One or more` `Two or more` `Three or more`\n          <dbl>         <dbl>           <dbl>\n1         0.294        0.0481         0.00543\n```\n\n\n:::\n:::\n\n\n\n\nA Poisson model with a $\\lambda$ of 0.347 (a little more than once every three years) gives similar\nresults to the Poisson log-normal model for the largest firms for one or more breaches, but\nunderestimates the likelihood for 2+ and 3+.\n\n# Simulate Improvement\n\nUsing the Poisson model, it is easy to simulate improvements.\n\nLet's assume that the firm implements a new security control that reduces the likelihood of a breach\nby 25%, and compares the number of breaches in the 5 years before and after implementing that\ncontrol. We then simulate this scenario 100,000 times:    \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulation <- function(years, reduction, runs = 1e5) {\n  tibble(\n    base = replicate(runs, sum(rpois(years, base_lambda))),\n    new = replicate(runs, sum(rpois(years, base_lambda * (1 - reduction))))\n  ) |>\n    mutate(\n      better = .data$new < .data$base, worse = .data$new > .data$base,\n      same = .data$new == .data$base\n    )\n}\n\nresults <- simulation(5, 0.25)\nhead(results, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 5\n    base   new better worse same \n   <int> <int> <lgl>  <lgl> <lgl>\n 1     4     1 TRUE   FALSE FALSE\n 2     2     1 TRUE   FALSE FALSE\n 3     2     1 TRUE   FALSE FALSE\n 4     3     2 TRUE   FALSE FALSE\n 5     1     2 FALSE  TRUE  FALSE\n 6     0     3 FALSE  TRUE  FALSE\n 7     2     2 FALSE  FALSE TRUE \n 8     1     1 FALSE  FALSE TRUE \n 9     2     3 FALSE  TRUE  FALSE\n10     2     0 TRUE   FALSE FALSE\n```\n\n\n:::\n:::\n\n\n\n\nOK, what are the results?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarize_results <- function(data) {\n  data |>\n    summarize(\n      better = sum(.data$better) / n(), worse = sum(.data$worse) / n(),\n      same = sum(.data$same) / n()\n    )\n}\n\nsummarize_results(results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  better worse  same\n   <dbl> <dbl> <dbl>\n1  0.478 0.288 0.235\n```\n\n\n:::\n:::\n\n\n\n\nFor this scenario, even though the true rate of breaches has been reduced by 25%, the number of\nbreaches in the 5 years after implementing the control is better less than 50% of the time, and is\nactually worse nearly 30% of the time!\n\n# Larger Sample\n\nTo improve the likelihood of detecting the improvement, we can adjust the scenario, taking a larger\nsample of years before and after implementing the new control. How many years would we need to get\na confident measurement of improvement?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulation(10, 0.25) |>\n  summarize_results()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  better worse  same\n   <dbl> <dbl> <dbl>\n1  0.556 0.287 0.157\n```\n\n\n:::\n\n```{.r .cell-code}\nsimulation(50, 0.25) |>\n  summarize_results()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  better worse   same\n   <dbl> <dbl>  <dbl>\n1  0.757 0.189 0.0542\n```\n\n\n:::\n\n```{.r .cell-code}\nsimulation(250, 0.25) |>\n  summarize_results()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  better  worse    same\n   <dbl>  <dbl>   <dbl>\n1  0.958 0.0358 0.00665\n```\n\n\n:::\n:::\n\n\n\n\nOnly the simulation which tests 250 years before and after (a total of 500 years!) gives a somewhat\nreasonable indication of improvement - and keep in mind that \"better\" is a reduction of *any*\namount. Running metrics for a period this long at a single firm is impractical, which leads to our\nconclusion...\n\n# Conclusion\n\nEven in a \"best case\" scenario of a very large firm that is able to measure the number of breaches\nover a 10 year period, a 25% reduction in breach rate only results in a lower number of actual\nbreaches less than 50% of the time. A sample period of 500 years or more is needed to have\nconfidence that the breach rate has been reduced.\n\nRealistically, this means that the only way to test the effectiveness of security controls is by\nsampling across large numbers of firms. A single organization simply won't have enough data to\nmake reasonable conclusions based on events that happen so infrequently.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}