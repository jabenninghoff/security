{
  "hash": "e3b66a05956c84f5796c9c1bf0756c90",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Measuring Incident Duration\"\nauthor: \"John Benninghoff\"\ndate: '2025-03-19'\ndate-modified: '2025-04-03'\ncategories: SRE\norder: 107\noutput:\n  html_notebook:\n    theme:\n      version: 5\n      preset: bootstrap\n    css: assets/extra.css\n    pandoc_args: --shift-heading-level-by=1\n    toc: yes\n    toc_float:\n      collapsed: no\n      smooth_scroll: no\n---\n\n\n\n\nAnalysis to determine thresholds of sample size to reliably detect changes in incident impact (duration) using Monte Carlo simulation.\n\n# Questions/TODO\n\n- [ ] Questions/TODO list here\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quantrr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(dplyr)\n```\n:::\n\n\n\n\n# Background\n\nIn 2021, Google published a report exploring the limitations of using Mean Time to Resolve (MTTR), [Incident Metrics in SRE: Critically Evaluating MTTR and Friends](https://sre.google/resources/practices-and-processes/incident-metrics-in-sre/). Since then,\nadditional articles and reports have been published that are also critical of MTTR and TTR in\ngeneral, explicitly advocating that\n\"[organizations should stop using MTTX, or TTx data in general](https://resilienceinsoftware.org/news/1157532)\". From my experience with cybersecurity\nincident data, I believe it is a mistake to abandon measurements of incidents entirely, and instead\nask the question, *under what conditions does incident duration indicate improvement?*\n\nKey observations and arguments from the 2021 Google paper include:\n\n- Incident duration appears to be lognormal (the paper did not test if the lognormal distribution\n  was the best fit)\n- A Monte Carlo simulation comparing two samples of incidents, one with no change in duration, and\n  a second with a 10% reduction in duration, MTTR did not reliably indicate the improvement\n- Improvements in MTTR can be observed even when there is no change in the underlying distribution\n- Larger sample sizes (more incidents) make detecting the change more reliable - in the original\n  simulation, the sample sizes ranged from approximately 50-300\n- Median and 95th percentile don't work well either\n- Geometric mean does just work, but only with a sample size of 500+\n- Measuring incident duration doesn't account for changes in incident frequency\n\nThe article also calls out some exceptions where measuring duration is effective: first, when your\ndata is large enough, as with Backblaze statistics on hard disk reliability, and second, when the\nchange in duration is large enough, such as an 80% reduction in duration.\n\nSubsequent reports from [VOID](https://www.thevoid.community) referenced the Google paper and argued:\n\n- Incident duration is long-tailed and appear lognormal (2021, 2022)\n- \"MTTx are Shallow Data\", an oversimplification of a complex, messy event (2021)\n- Organizations should stop using MTTx and TTx data (2021, 2022, 2024)\n- Incident duration appears lognormal for some organizations, but not for others (2022)\n- Replicated the Monte Carlo simulation from the Google paper (2022)\n\nIn the cybersecurity domain, the [Cyentia Institute](https://www.cyentia.com) has published reports\nanalyzing breach data, looking at both frequency and impact (cost). Their\n[IRIS 2022](https://www.cyentia.com/wp-content/uploads/IRIS-2022_Cyentia.pdf) report found that,\nlike outage incident duration, security breach losses closely follow a lognormal distribution. Like\nthe Google and VOID reports, the IRIS also points out that the mean is a poor indicator of a\n'typical' loss, the median or geometric mean are better summaries, and organizations should also be\nconcerned with tail risk - extreme events at the 90, 95, and 99th percentile.\n\nAdditionally, despite the challenges with using lognormal data, the\n[2024 Ransomware Report](https://www.cyentia.com/iris-ransomware/) showed that losses from\nransomware have significantly increased between 2019 and 2023.\n\n# Analysis\n\nWhile I agree that the mean (average) is misleading and should not be used with incident impact data\n(duration or costs), I believe it is a mistake to stop using impact data entirely. A Monte Carlo\nsimulation can help answer the underlying question raised by the Google research, \"under what\nconditions does incident duration indicate improvement?\" Put differently, how large of a sample size\nis needed to detect a given level of improvement (reduction) in incident duration?\n\n## Assumptions\n\nWhile the 2022 VOID Report challenged the notion that incident duration is lognormal, the Google\nreport and my own experience suggest otherwise: similar to cybersecurity impact, incident duration\nclosely follows a lognormal distribution. In my own work, I have found that incidents that are\nqualitatively different may not be lognormal, and that could well be what was happening in the VOID\nreport, but we lack the context to know for sure. To simplify the analysis, the incident frequency\nwill be held constant. (Side note: Cyentia found that cybersecurity breach frequency followed a\nPoisson lognormal distribution)\n\n## Approach\n\nTo determine the threshold of incidents needed to detect changes in duration, we follow a similar\nmethodology as the Google paper:\n\n1. Randomly sample from an artificial but representative lognormal distribution of incident duration\n2. Adjust the duration of one of the populations by a set percentage to simulate improvement\n3. Determine if there is a statistically significant improvement in incident duration\n\nFor step 3, we use the geometric mean and\n[Student's t-test](https://en.wikipedia.org/wiki/Student's_t-test) on the samples, after\nlog-transforming the data. From my research and understanding, this is a reasonable approach if the\nunderlying distributions are truly lognormal, which is true by design.\n\nTo determine a representative distribution, we can borrow data from the Google report, which lists\nthe p95 and p50 for three companies - we'll infer p05 and the lognormal parameters using data from\nCompany B:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlnorm_param(9, 528, 67)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$meanlog\n[1] 4.23316\n\n$sdlog\n[1] 1.237761\n\n$mdiff\n[1] -0.02806642\n```\n\n\n:::\n:::\n\n\n\n\nUsing trial and error, we find that 9 minutes is the closest match to p05 when the median is 67\nminutes. From this, we can create a representative distribution:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeanlog <- 4.25\nsdlog <- 1.25\nqlnorm(c(0.05, 0.5, 0.95), meanlog = meanlog, sdlog = sdlog)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   8.970424  70.105412 547.885889\n```\n\n\n:::\n:::\n\n\n\n\nThis is reasonably close to the Company B data, and passes the sniff test: 5% of incidents resolved\nin less than 9 minutes, 50% resolved in 70 minutes or less, and 95% resolved under 548 minutes (9\nhours 8 minutes).\n\n## Simulations\n\nFirst, test at the 10% improvement level, with a sample size ($N$) of 100, which is about 1 year's\nworth of incidents. A single test is successful if the experimental group has a lower geometric mean\nand a positive t-test (p < 0.05) on the log-transformed data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsingle_test <- function(n, improvement) {\n  control <- rlnorm(n, meanlog = meanlog, sdlog = sdlog)\n  experiment <- rlnorm(n, meanlog = meanlog, sdlog = sdlog) * (1 - improvement)\n\n  list(gmean(control), gmean(experiment), t.test(log(control), log(experiment)))\n}\n\nsingle_test(100, 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 59.03588\n\n[[2]]\n[1] 61.86311\n\n[[3]]\n\n\tWelch Two Sample t-test\n\ndata:  log(control) and log(experiment)\nt = -0.24151, df = 197.93, p-value = 0.8094\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.4287449  0.3351876\nsample estimates:\nmean of x mean of y \n 4.078145  4.124924 \n```\n\n\n:::\n:::\n\n\n\n\nFor most tests, the p-value is well above 0.05. How often does the test succeed? We'll set the bar\nslightly higher than the Google paper and target a 95% success rate.\n\nWe'll use a slightly different function that is more easily converted into a data frame, and run\nonly 10,000 trials to speed things up:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_change <- function(n, improvement) {\n  control <- rlnorm(n, meanlog = meanlog, sdlog = sdlog)\n  experiment <- rlnorm(n, meanlog = meanlog, sdlog = sdlog) * (1 - improvement)\n\n  c(\n    control_gmean = gmean(control), experiment_gmean = gmean(experiment),\n    p_value = t.test(log(control), log(experiment))$p.value\n  )\n}\n\nsimulate_test <- function(n, improvement, trials = 10000, p_value_target = 0.05) {\n  map(1:trials, \\(i) as_tibble_row(test_change(n, improvement))) |>\n    list_rbind() |>\n    mutate(\n      success = (.data$control_gmean > .data$experiment_gmean) & (.data$p_value < p_value_target)\n    )\n}\n\nresults <- simulate_test(100, 0.1)\nhead(results, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 Ã— 4\n   control_gmean experiment_gmean p_value success\n           <dbl>            <dbl>   <dbl> <lgl>  \n 1          89.5             68.8  0.132  FALSE  \n 2          69.3             53.4  0.136  FALSE  \n 3          80.3             68.9  0.389  FALSE  \n 4          88.2             56.9  0.0141 TRUE   \n 5          64.7             60.6  0.706  FALSE  \n 6          63.7             78.1  0.250  FALSE  \n 7          75.1             75.8  0.959  FALSE  \n 8          83.3             69.3  0.266  FALSE  \n 9          68.4             67.8  0.963  FALSE  \n10          72.6             60.2  0.289  FALSE  \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(results$success)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0846\n```\n\n\n:::\n:::\n\n\n\n\nFor a sample size of 100, we successfully detect a statistically significant change about 8% of the\ntime. We can try larger sample sizes:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_test(1000, 0.1) |>\n  pull(success) |>\n  mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4682\n```\n\n\n:::\n\n```{.r .cell-code}\nsimulate_test(2000, 0.1) |>\n  pull(success) |>\n  mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7617\n```\n\n\n:::\n\n```{.r .cell-code}\nsimulate_test(4000, 0.1) |>\n  pull(success) |>\n  mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9644\n```\n\n\n:::\n:::\n\n\n\n\nA sample size of 4000 is needed to detect a significant change. For a single organization, it would\ntake about 80 years (40 before, 40 after) to gather the data. However, the same volume of data could\nbe gathered from 80 different organizations (40 in each group) in a year.\n\nWe can also try larger changes:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_test(100, 0.25) |>\n  pull(success) |>\n  mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3584\n```\n\n\n:::\n\n```{.r .cell-code}\nsimulate_test(100, 0.50) |>\n  pull(success) |>\n  mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9752\n```\n\n\n:::\n:::\n\n\n\n\nInterestingly, while the Google paper notes that it would take a very large change (80% reduction)\nto be detected through measuring incident duration, we find that a 50% reduction is clearly\ndetectable over two years of data.\n\nFor the 50% reduction, what is the range of the observed improvements?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_test(100, 0.50) |>\n  mutate(change = experiment_gmean / control_gmean) |>\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n control_gmean    experiment_gmean    p_value           success       \n Min.   : 43.80   Min.   :19.51    Min.   :0.0000000   Mode :logical  \n 1st Qu.: 64.42   1st Qu.:32.26    1st Qu.:0.0000068   FALSE:264      \n Median : 70.17   Median :35.12    Median :0.0001205   TRUE :9736     \n Mean   : 70.71   Mean   :35.34    Mean   :0.0056374                  \n 3rd Qu.: 76.24   3rd Qu.:38.14    3rd Qu.:0.0014284                  \n Max.   :114.56   Max.   :57.94    Max.   :0.6738022                  \n     change      \n Min.   :0.2616  \n 1st Qu.:0.4435  \n Median :0.5001  \n Mean   :0.5078  \n 3rd Qu.:0.5641  \n Max.   :0.9259  \n```\n\n\n:::\n:::\n\n\n\n\nThe observed improvements range from a small increase/decrease to a 75% reduction, with half falling\nwithin 6% of the true decrease (44-56%).\n\n# Discussion\n\nSo, what does this analysis tell us?\n\n1. MTTR (Mean Time to Resolve) is a misleading metric, and organizations should stop using it.\n\nThe Google, VOID and Cyentia research all agree that the mean (average) is a poor summary metric for\nlognormal and other long-tail distributions. I also agree and that assumption is built-in to the\nanalysis. However, I agree with the Cyentia research that the geometric mean is a reasonable value\nfor the 'typical' incident - this is suggested in the Google paper.\n\n2. Large samples are required to detect changes in underlying incident duration, making it less\n   useful at an organizational level.\n\nThe Google paper found that sample sizes of 500+ were needed to detect a 10% reduction in the\nunderlying distribution; this analysis suggested 4000 is a better threshold. Even at organizations\nwith high levels of incidents, this represents several years of data, enough that only large changes\n(50% or greater reduction in underlying TTR) will be detected in a reasonable time frame.\n\n3. TTR (incident duration) can be a useful measure at a larger scale, including comparisons across\n   different organizations.\n\nWhile at an individual organization level, incident duration (TTR) is noisy, it becomes meaningful\nat large scales. Some organizations *do* experience repeatable failures at scale, including the\nBackblaze hard drive failure example cited in the Google paper, but TTR is more useful for\ncomparisons across organizations. I believe this explains why the Accelerate State of DevOps Report\n([DORA](https://dora.dev)) research is able to find meaningful correlations between \"Failed\ndeployment recovery time\", the other 3 key DORA metrics, and other measures of performance. The 2024\nreport surveyed 3,000 people, which should represent hundreds (or more) different organizations.\nThis would appear to be above a threshold to detect differences, especially given the range of\npossible responses (1 hour, 1 day, 1 week, 1 month, 6 months, 1 year).\n\nIn the cybersecurity space, large (public) incidents happen relatively infrequently, about once\nevery 3-10 years, depending on the size of the organization, according to the Cyentia IRIS report.\nBecause of this, metrics on breaches gathered at the organizational level don't provide meaningful\ninsights, as I argued in [Measuring Changes in Breach Rates](breach-rates.Rmd). However, we've also\nlearned that looking at data across organizations can and does provide useful learning. We can\naccomplish the same thing in the reliability/resilience space by using similar measures.\n\nTTR is a useful measure for cross-company comparison *because* it is a shallow measure, and reflects\nthe external consumer/user experience. One study I'd like to see is a deep-dive analysis on data\nfrom [Downdetector](https://downdetector.com), examining differences in groups or clusters of\norganizations (perhaps this has already been done!) which could identify common factors that\ncontribute to success.\n",
    "supporting": [
      "measuring-incidents_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}